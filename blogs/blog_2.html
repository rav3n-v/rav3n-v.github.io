<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-B32571Q9D0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-B32571Q9D0');
</script>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description" content="Optimisation Techniques.">
    <meta name="date" content="June 29, 2025">
  <title>Deep Learning - Optimisation Techniques</title>
  <link rel="stylesheet" href="blog_styles_2.css" />
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>

<body class="dark-theme">
  <header role="banner">
    <div class="header-container">
      <h1>Deep Learning</h1>
      <nav role="navigation">
        <a href="../index.html" class="nav-link" aria-label="Back to Home">Home</a>
      </nav>
    </div>
       <p class="subtitle">Mathematics behind AI</p>

  </header>

  <main role="main">
    <aside class="in-page-navigation" role="navigation" aria-label="Main Navigation">
    <h3 class="in-page-nav-heading">On this page</h3>
    <ul>
        <li><a href="#post-title">Optimisation Techniques</a></li>
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#gradient-descent">Gradient Descent</a></li>
      <li><a href="#Momentum">Momentum-Based Optimizer</a></li>
      <li><a href="#references">References</a></li>


    </ul>
  </aside>
    <article role="article" aria-labelledby="post-title" class="content">
      <h2 id="post-title">Optimisation Techniques</h2>
      <p class="meta">Posted on June 28, 2025</p>
      <div class="svg-container" style="background-color: #1e1e1e"><img viewBox="0 0 2000 800" src="Images/Blog_2_Images/pringle.svg"></div>

      <h2 id="introduction">Introduction</h2>
      <p>Optimization refers to the process of finding the best solution within given constraints.
        In the context of neural networks, it means finding the optimal set of weights that minimizes the loss function.
        Our goal is not just to find weights that perform well on the training data,
        but also those that generalize well to unseen data.
      </p>
      <h2 id="gradient-descent">Gradient Descent</h2>
      <p>Gradient Descent is a fundamental optimization algorithm that uses the concept of a slope to minimize
        the loss function.
        <p>Let's take the function f(x)=\(x^2\)</p>
      <div class="svg-container" style="background-color: #1e1e1e"><img viewBox="0 0 2000 800" src="Images/Blog_2_Images/x^2_plot.svg"></div>

        <p>The derivative of a function indicates the direction of the steepest ascent,
        so we move in the opposite direction to reduce the loss.</p>
      <p>From the figure above, if we start at x = 3 then y = 9.In order to make y as zero,
        we need to move in the direction opposite to the x-axis</p>
      <div class="math">
        \begin{align}
        &f(x+a) = f(a)+ hf'(a) + h^2 \frac{f''(a)}{2!} \quad &\text{(Taylor series approximation)}\\
        &f(x+a)-f(a) = h*f'(a) \quad &\text{(First order approximation)} \\
        \end{align}

      </div>
      <p>
      We need to move in the direction where the function value decreases. That is \(f(x+a)-f(a)\) < 0
      We can take h=-f'(a) and move in the direction of the steepest ascent. Which will guarantee that the function value is decreasing.
        That's how the gradient descent formula was derived.</p>


      <p>We compute the partial derivatives with respect to the weights,
        which tells us how to update each weight to decrease the loss.</p>
      <p>There are many types of loss functions—like Mean Squared Error (MSE) and Cross-Entropy Loss—but in general,
         loss function assigns low values(or preferablly zero) when predictions match targets and high positive values when they differ.</p>
        <p>The gradient descent update rule is:
      </p>
      <div class="math">
        \[W_{i+1} = W_i - \alpha \frac{\partial L}{\partial W_i}\]
      </div>
      <p>
        where \(\alpha\) is the learning rate. One limitation of standard gradient descent is that it uses a constant step size,
        which may be inefficient in regions where the landscape of the loss function changes rapidly.
      </p>
      <h2 id="Momentum">Momentum-Based Optimizer</h2>
      <p>To address the issue of slow convergence in standard gradient descent, momentum-based optimizers incorporate
      the idea of momentum from physics.
      These optimizers consider not only the current gradient but also the accumulated past gradients,
      allowing the optimizer to "build speed" in directions with consistent descent.</p>
      <div class="math">
        \[v_t = \beta v_{t-1} + \alpha \frac{\partial L}{\partial W_t}\]
        \[W_t = W_{t-1} - v_t\]
      </div>


      <h2 id="references">References</h2>
      <ol>
  <li id="ref1">
    Charu C. Aggarwal, <em>Linear Algebra and Optimization for Machine Learning</em>. Springer Cham, 2020.
    <a href="https://doi.org/10.1007/978-3-030-40344-7" target="_blank" rel="noopener noreferrer">https://doi.org/10.1007/978-3-030-40344-7</a>
  </li>
        <li id="ref2">
    Goodfellow, I., Bengio, Y., & Courville, A. (2016).
    <em>Deep Learning</em>. MIT Press.
    <a href="http://www.deeplearningbook.org" target="_blank" rel="noopener noreferrer">
      http://www.deeplearningbook.org
    </a>
  </li>

</ol>
    </article>

  </main>

  <footer role="contentinfo">
    <p>
  © 2025 Vinayak Devesan. Content licensed under
  <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener noreferrer">
    CC BY 4.0
  </a>.
</p>
  </footer>
</body>

</html>
