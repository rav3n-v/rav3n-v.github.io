<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-B32571Q9D0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-B32571Q9D0');
</script>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description" content="A beginner-friendly introduction to neural networks with SVG visualization and mathematical explanation.">

  <title>Deep Learning -Neural Network</title>
  <link rel="stylesheet" href="../styles_blog.css" />
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>

<body class="dark-theme">
  <header role="banner">
    <div class="header-container">
      <h1>Deep Learning</h1>
      <nav role="navigation">
        <a href="../index.html" class="nav-link" aria-label="Back to Home">Home</a>
      </nav>
    </div>
    <p class="subtitle">Mathematics behind AI</p>
  </header>

  <main role="main">
    <aside class="toc" role="complementary" aria-label="Navigation">
    <h3>On this page</h3>
    <ul>
      <li><a href="#post-title">Neural Network</a></li>
      <li><a href="#introduction">Introduction</a></li>
      <li><a href="#forward-propagation">Forward Propagation</a></li>
      <li><a href="#learning">How Neural Networks Learn?</a></li>
      <li><a href="#loss">Loss Function</a></li>
      <li><a href="#activation">Activation Function</a></li>
      <li><a href="#backpropagation">Back Propagation</a></li>
    </ul>
  </aside>
    <article role="article" aria-labelledby="post-title" class="content">
      <h2 id="post-title">Neural Network</h2>
      <p class="meta">Posted on June 22, 2025</p>
      <div class="svg-container">
      <svg viewBox="0 0 2000 800"  xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="none">
        <g transform="translate(-1616.6807483614048,-896.3465216984839) scale(2.794085262605705)"><path class="link" marker-end="" d="M800,365.5, 980,465.5" style="stroke-width: 0.93; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;"/><path class="link" marker-end="" d="M800,405.5, 980,465.5" style="stroke-width: 0.93; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;"/><path class="link" marker-end="" d="M800,445.5, 980,465.5" style="stroke-width: 0.93; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;"/><path class="link" marker-end="" d="M800,485.5, 980,465.5" style="stroke-width: 0.93; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;"/><path class="link" marker-end="" d="M800,525.5, 980,465.5" style="stroke-width: 0.93; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;"/><path class="link" marker-end="" d="M980,425.5, 1160,445.5" style="stroke-width: 0.93; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;"/><path class="link" marker-end="" d="M980,465.5, 1160,445.5" style="stroke-width: 0.93; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;"/><path class="link" marker-end="" d="M800,365.5, 980,425.5" style="stroke-width: 0.93; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;"/><path class="link" marker-end="" d="M800,405.5, 980,425.5" style="stroke-width: 0.93; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;"/><path class="link" marker-end="" d="M800,445.5, 980,425.5" style="stroke-width: 0.93; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;"/><path class="link" marker-end="" d="M800,485.5, 980,425.5" style="stroke-width: 0.93; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;"/><path class="link" marker-end="" d="M800,525.5, 980,425.5" style="stroke-width: 0.93; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;"/><circle r="10" class="node" id="0_0" cx="800" cy="365.5" style="fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);"/><circle r="10" class="node" id="0_1" cx="800" cy="405.5" style="fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);"/><circle r="10" class="node" id="0_2" cx="800" cy="445.5" style="fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);"/><circle r="10" class="node" id="0_3" cx="800" cy="485.5" style="fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);"/><circle r="10" class="node" id="0_4" cx="800" cy="525.5" style="fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);"/><circle r="10" class="node" id="1_0" cx="980" cy="425.5" style="fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);"/><circle r="10" class="node" id="1_1" cx="980" cy="465.5" style="fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);"/><circle r="10" class="node" id="2_0" cx="1160" cy="445.5" style="fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);"/><text class="text" dy=".35em" x="765" y="565.5" style="font-size: 12px;">Input Layer ∈ ℝ⁵</text><text class="text" dy=".35em" x="945" y="565.5" style="font-size: 12px;">Hidden Layer ∈ ℝ²</text><text class="text" dy=".35em" x="1125" y="565.5" style="font-size: 12px;">Output Layer ∈ ℝ¹</text></g><defs><marker id="arrow" viewBox="0 -5 10 10" markerWidth="7" markerHeight="7" orient="auto" refX="40"><path d="M0,-5L10,0L0,5" style="stroke: rgb(80, 80, 80); fill: rgb(80, 80, 80);"/></marker></defs></svg>
      </div>
      <h2 id="introduction">Introduction</h2>
        <p>Neural Networks are a fundamental concept in deep learning.
          The structure above takes in 5 inputs and produces a single output helping us classify data with 5 features.
          This is a very rudimentary structure of Neural Network, but let's get started with this</p>
      <h2 id="forward-propagation">Forward Propagation</h2>
      <p> Mathematics behind this structure is very interesting. Let's call our input as X, hidden layer as H, and output as Y.</p>
      <div class="math">
        \[X= \begin{bmatrix}
        x_1 \\
        x_2 \\
        x_3 \\
        x_4 \\
        x_5 \\
        \end{bmatrix} \quad
        H= \begin{bmatrix}
        h_1 \\ h_2  \end{bmatrix} \quad

        Y= \begin{bmatrix}
        y_1 \end{bmatrix} \]
      </div>
      <p>We can write the equation as</p>
      <div class="math">
        \[Y= W_2^TH = W_1^TX\]
        </div>
        <p>Where W_1 is the weight matrix for the input layer, W_2 is the weight matrix for the hidden layer, and X is the input vector.
          Dimension of W_1 and W_2 are 5x2 and 2x1 respectively.</p>
  <div class="math">
    \[ W_1 = \begin{bmatrix}
    w_{11} & w_{12} \\
    w_{21} & w_{22} \\
    w_{31} & w_{32} \\
    w_{41} & w_{42} \\
    w_{51} & w_{52} \\
    \end{bmatrix} \quad
    W_2 = \begin{bmatrix}
    w_{11} \\ w_{12} \end{bmatrix} \]
  </div>
      <p>If you're familiar with matrix multiplication, you might wonder why we need a hidden layer at all?. We could have just used a single layer neural network.
        Yes we can. To get more complex structures, we use an activation function. Therefore, turning our linear equation into a non-linear equation.</p>
      <h2 id ="activation">Activation Function</h2>
      Let's take a simple example of tanh activation function.
      <div class="math">
        \[tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\]
      </div>
      <p>Applying this to the hidden layer gives us.</p>
      <div class="math">
        \[H = tanh(W_1^TX) = \begin{bmatrix}
        tanh(w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + w_{14}x_4 + w_{15}x_5) \\
        tanh(w_{21}x_1 + w_{22}x_2 + w_{23}x_3 + w_{24}x_4 + w_{25}x_5) \\
        \end{bmatrix} \]
      </div>
      <p>
      This is why we use an activation function. To turn our linear equation into a non-linear equation. Here’s a challenge for the reader,
        try to find the matrix that could have converted our input to the output layers while using only 1 weight matrix bypassing the hidden layer.</p>
      <h2 id="learning">How do Neural Networks Learn?</h2>
      <p>Now that we understand the structure of neural networks, let’s explore how they learn.</p>
      <p>Learning in a neural network is about finding the optimal set of weights that produce the correct output for a given input.
        To achieve this, the network is trained on a <strong>dataset</strong> that contains inputs along with their corresponding correct outputs.
        This is known as the <strong>training dataset</strong>.</p>

      <p>By comparing the network’s predictions to the actual outputs in the training data, we adjust the weights in such a way that the error is minimized.
        In essence, learning is the process of tuning the weights so that the network performs well on the given task.</p>
      <h2 id="loss">Loss Function</h2>
      <p>Loss Functions are used to compare the “distances” between the target values and output
of Neural Network. There are a wide variety of loss functions. We will see use MSE(Mean squared Error) for now</p>
      <div class="math">
        \[MSE = \frac{1}{2n}\sum_{i=1}^{n}(y_i - \hat{y_i})^2\]
      </div>
      <p>Where n is the number of training examples, \(y_i\) is the actual output and \(\hat{y_i}\) is the predicted output.</p>


      <h2 id="backpropagation">Backpropagation</h2>
      <p>Backpropagation is the process of calculating the gradient of the loss function with respect to the weights.
        This is done by iteratively calculating the gradient of the loss function with respect to each weight and then updating the weights accordingly.</p>
      <p>The gradient of the loss function with respect to a weight \(w_i\) is given by</p>
      <div class="math">
        \[dW_i = \frac{\partial L}{\partial w_i} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y_i})\frac{\partial}{\partial w_i}y_i\]
      </div>
      <h2>Gradient Descent</h2>
      <p>Gradient descent is a simple algorithm for optimizing a loss function.
        It is a simple iterative process that starts with an initial set of weights and iteratively updates the weights based on the gradient of the loss function.</p>
      <div class="math">
        \[w_{i+1} = w_i - \alpha \frac{\partial L}{\partial w_i}\]

      </div>
        <p>
      where \(w_i\) is the weight at iteration \(i\) and \(\alpha\) is the learning rate.
      Gradient descent is a comparitively simple weight update rule.</p>
        <h3>Why does this work?</h3>
        <p>
        If you recall from high school math, the gradient of a function represents its slope at a given point.
        In the context of neural networks, the gradient of the loss function with respect to a weight tells us how the loss changes as that weight changes.
        The gradient points in the direction of the steepest increase in the loss.
        So, to minimize the loss, we move in the opposite direction — this is the essence of gradient descent.
        By repeatedly updating the weights in the direction that reduces the loss, the network gradually learns the optimal values.
      </p>
        <div class="image-row">
  <img src="Blog_1_images/frame_000.png" alt="Description 1">
  <img src="Blog_1_images/frame_010.png" alt="Description 2">
  <img src="Blog_1_images/frame_029.png" alt="Description 3">
</div>
        <p>Try calculating the gradients for the parabolas above and visualising the gradient descent approach.</p>
        <p>We’ll explore more advanced weight update rules and loss functions in upcoming blog posts.</p>



      <h2>References</h2>
      <ol>
  <li id="ref1">
    Charu C. Aggarwal, <em>Linear Algebra and Optimization for Machine Learning</em>. Springer Cham, 2020.
    <a href="https://doi.org/10.1007/978-3-030-40344-7" target="_blank" rel="noopener noreferrer">https://doi.org/10.1007/978-3-030-40344-7</a>
  </li>
        <li id="ref2">
    Goodfellow, I., Bengio, Y., & Courville, A. (2016).
    <em>Deep Learning</em>. MIT Press.
    <a href="http://www.deeplearningbook.org" target="_blank" rel="noopener noreferrer">
      http://www.deeplearningbook.org
    </a>
  </li>

</ol>
    </article>

  </main>

  <footer role="contentinfo">
    <p>
  © 2025 Vinayak Devesan. Content licensed under
  <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener noreferrer">
    CC BY 4.0
  </a>.
</p>
  </footer>
</body>

</html>
